# -*- coding: utf-8 -*-
"""Research_Question.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nqd2_Aj088vOhh04VzEbK0zL39Km-oJH

**Data augmentation on image dataset**

When doing the main tasks required for this portfolio coursework I always observed poor accuracy when trying to classify the images; never been able to reach much more than 80% accuracy on test. I was curious to see if I could find ways to increase the classifier accuracy on this dataset. To this end I've investigated data augmentation methods as well as increasing the number of layers in the model. I've introduced the methods individually then described the best combination I've identified.
"""

import matplotlib.pyplot as plt
import tensorflow as tf

!unzip -q Cat_Dog_or_Wild.zip

img_size = (300, 300)
batch_size = 32

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    "Cat_Dog_or_Wild",
    validation_split=0.2,
    subset="training",
    seed = 1337,
    image_size=img_size,
    batch_size=batch_size,
)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    "Cat_Dog_or_Wild",
    validation_split=0.2,
    subset="validation",
    seed = 1337,
    image_size=img_size,
    batch_size=batch_size,
)

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title("cat" if (int(labels[i])==0) else ("dog" if (int(labels[i])==1) else "wild"))
        plt.axis("off")

train_ds = train_ds.prefetch(buffer_size=32)
val_ds = val_ds.prefetch(buffer_size=32)

num_filters = 16
filter_size = 3
pool_size = 2

# Basic model
model = tf.keras.models.Sequential([
  tf.keras.layers.experimental.preprocessing.Rescaling(1.0 / 255),
  tf.keras.layers.Conv2D(num_filters, filter_size, input_shape=(img_size[0], img_size[1], 3)),
  tf.keras.layers.MaxPooling2D(pool_size=pool_size),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(3, activation='softmax'),
])

# Advanced model
model = tf.keras.models.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip("horizontal"),
  tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),
  tf.keras.layers.experimental.preprocessing.Rescaling(1.0 / 255),
  tf.keras.layers.Conv2D(num_filters, filter_size, input_shape=(img_size[0], img_size[1], 3)),
  tf.keras.layers.MaxPooling2D(pool_size=pool_size),
  tf.keras.layers.Activation("relu"),
  tf.keras.layers.Conv2D(num_filters, filter_size, input_shape=(img_size[0], img_size[1], 3)),
  tf.keras.layers.MaxPooling2D(pool_size=pool_size),
  tf.keras.layers.Activation("relu"),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(3, activation='softmax'),
])

epochs = 50
model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"],
)

history = model.fit(train_ds, epochs=epochs, validation_data=val_ds)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

for images, labels in val_ds.take(-1):
    for i in range(len(images)):
        augmented_image = tf.expand_dims(images[i], 0)
        img_array = tf.keras.preprocessing.image.img_to_array(augmented_image[0])
        img_array = tf.expand_dims(img_array, 0)
        predictions = model.predict(img_array)
        score = predictions[0]
        if ( max(score) < 0.5 ):
            plt.figure(figsize=(3, 3))
            plt.imshow(augmented_image[0].numpy().astype("uint8"))
            plt.axis("off")
            plt.show()
            print(score)